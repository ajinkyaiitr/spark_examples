------sample
sample(withReplacement,fraction,[seed])
$val seq = sc.parallelize(1 to 100, 5)
$seq.sample(false, 0.1).collect();
$seq.sample(true, 0.1).collect();


####Program to illustrate RDD
val seq = sc.parallelize(1 to 100,5)
seq.sample(true,0.1)
seq.sample(true,0.1).collect()

A Scala function declaration has the following form −

def functionName ([list of parameters]) : [return type]
Methods are implicitly declared abstract if you don’t use the equals sign and the method body.

Function Definitions
A Scala function definition has the following form −

Syntax
def functionName ([list of parameters]) : [return type] = {
   function body
   return [expr]



###Defining a function in RDD 
def f(l:Iterator[Int]):Iterator[Int] = {
 var sum = 0
 while(l.hasNext){
 sum = sum + l.next
 }
 return List(sum).iterator
}



seq.mapPartitions(f).collect()
------- Returns a new RDD by applying a function to each partition of this RDD


sortBy(func,ascending=True,numPartitions=None)
-----Sorts the RDD by the given function
---Creation of an RDD
var rdd = sc.parallelize(temp)
----Sorting an RDD by 1st Element
rdd.sortBy(x=>x._1).collect()
----Sorting an RDD by a second element
rdd.sortBy(x=>x._2).collect()
----Sorting an entire element
rdd1.sortBy(x=>x).collect()
-----Distinct elements in an RDD
rdd3.distinct.collect()
-----Subtract elemeents of the 2 rdds
rdd1.subract(rdd2)
-----Intersection of the 2 rdds
rdd1.intersect(rdd2)
-----Cartesian of the 2 RDD
rdd1.cartesian(rdd2)

----Fold Method in Spark
var myrdd = sc.parallelize(1 to 10,2)
var myrdd1 = myrdd.map(_.toString)
def concat(s:string,n:string):String = s + n
var s = "_"
myrdd1.fold(s)(concat)

------Aggregate method in Spark

var rdd11 = sc.parallelize(1 to 100)
var init = (0,0)
def seq(t1:(Int,Int),i:Int):(Int,Int) = (t._1 + i,t._2 + 1)
def comb(t1:(Int,Int),t2:(Int,Int)):(Int,Int) = (t1._1 + t2_2,t2_1 + t2_2)
var d = rdd.aggregate(init)(seq,comb)

-----Ordering Method in Spark

sc.parallelize(List(10, 1, 2, 9, 3, 4, 5, 6, 7)).takeOrdered(6)
var l = List((10, "SG"), (1, "AS"), (2, "AB"), (9, "AA"), (3, "SS"), (4, "RG"), (5, "AU"), (6, "DD"), (7, "ZZ"))
var r = sc.parallelize(l)
r.takeOrdered(6)(Ordering[Int].reverse.on(x => x._1))
(10,SG), (9,AA), (7,ZZ), (6,DD), (5,AU), (4,RG)
r.takeOrdered(6)(Ordering[String].reverse.on(x => x._2))
(7,ZZ), (3,SS), (10,SG), (4,RG), (6,DD), (5,AU)
r.takeOrdered(6)(Ordering[String].on(x => x._2))
(9,AA), (2,AB), (1,AS), (5,AU), (6,DD), (4,RG)

======Top Method
var a=sc.parallelize(List(4,4,8,1,2, 3, 10, 9))
a.top(6)

======For Each Method
Applies a function to each element of the RDD
>>> def f(x:Int)= println(s"Save $x to DB")
>>> sc.parallelize(1 to 5).foreach(f)


=====Foreach Method on partion
def partitionSum(itr: Iterator[Int]) =
println("The sum of the parition is " + itr.sum.toString)
sc.parallelize(1 to 40, 4).foreachPartition(partitionSum)
The sum of the parition is 155
The sum of the parition is 55
The sum of the parition is 355
The sum of the parition is 255

======Program for word count=======
val lines = sc.textFile("/data/mr/wordcount/big.txt")
val words = lines.flatMap(x => x.split(" "))
val pairs = words.map(s => (s.toLowerCase(), 1))
val counts = pairs.reduceByKey((a, b) => a + b)
counts.take(10)
//counts.saveAsTextFile("word-count-spark");

=====Program for calculating maximum temperature======
var txtRDD = sc.textFile("/data/spark/temps.csv")

def cleanRecord(line:String) = {
    var arr = line.split(",");
    (arr(1).trim, arr(0).toInt)
}
var recordsRDD = txtRDD.map(cleanRecord)

def max(a:Int, b:Int) = if (b > a) b else a

var result = recordsRDD.reduceByKey(max)
result.collect()







